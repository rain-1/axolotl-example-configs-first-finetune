base_model: NousResearch/Llama-3.2-1B
model_type: LlamaForCausalLM
tokenizer_type: AutoTokenizer
trust_remote_code: true

load_in_4bit: true
#adapter: qlora
adapter: lora
lora_model_dir:

sequence_len: 2048
sample_packing: true
eval_sample_packing: true

# These are the key changes for vLLM compatibility:
# 1. `peft_use_dora` is either `false` or not present.
# 2. `lora_modules_to_save` is not present.
peft_use_dora: false

lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target_linear: true

datasets:
  - path: winglian/pirate-ultrachat-10k
    type: chat_template
    chat_template: llama3

val_set_size: 0.005
sequence_len: 1024
sample_packing: true
pad_to_sequence_len: true

output_dir: ./outputs-vllm-compat/lora-out

gradient_accumulation_steps: 2
micro_batch_size: 1
num_epochs: 2
optimizer: adamw_bnb_8bit
lr_scheduler: cosine
learning_rate: 0.0002

train_on_inputs: false
bf16: true
fp16: false
tf32: true

special_tokens:
  pad_token: "<|end_of_text|>"

save_strategy: steps
save_steps: 100
save_total_limit: 32

# trainable params: 11,272,192 || all params: 1,247,086,592 || trainable%: 0.9039
